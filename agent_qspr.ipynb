{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7ca916a",
   "metadata": {},
   "source": [
    "# QSPRpred API Integration with LangChain Agent\n",
    "\n",
    "This notebook demonstrates the integration of the BridgeDB API with a LangChain-powered AI agent for biological identifier mapping.\n",
    "\n",
    "## Table of Contents\n",
    "1. Setup and Imports\n",
    "2. Papyrus API Wrapper\n",
    "3. Custom LangChain Tool\n",
    "4. AI Agent Configuration\n",
    "5. Testing the Agent\n",
    "6. Additional Utility Functions\n",
    "7. Direct API Calls\n",
    "\n",
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's set up our environment and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5efaf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.tools import BaseTool\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from qsprpred.data.sources.papyrus import Papyrus\n",
    "import qsprpred\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set OpenAI API Key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"OpenAI API Key is not set. Please check your .env file.\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n",
    "dataset_name = \"PapyrusTutorialDataset\"  # name of the file to be generated\n",
    "papyrus_version = \"latest\"  # Papyrus database version\n",
    "data_dir = \"papyrus\"  # directory to store the Papyrus data\n",
    "output_dir = \"data\"  # directory to store the generated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb660bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from papyrus_scripts.download import download_papyrus\n",
    "# download_papyrus(version='latest', structures=True, descriptors=['mold2', 'unirep'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bc71fa",
   "metadata": {},
   "source": [
    "### Practice dataset\n",
    "\n",
    "First we'll create a test dataset for validating the results for the dataset retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52f173a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90054a07a2894242a67948b2cee2584c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3785"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qsprpred.data.sources.papyrus import Papyrus\n",
    "import qsprpred\n",
    "\n",
    "acc_keys = [\"P29274\"]\n",
    "dataset_name = \"PapyrusTutorialDataset\"  # name of the file to be generated\n",
    "quality = \"high\"  # choose minimum quality from {\"high\", \"medium\", \"low\"}\n",
    "papyrus_version = \"latest\"  # Papyrus database version\n",
    "data_dir = \"papyrus\"  # directory to store the Papyrus data\n",
    "output_dir = \"data\"  # directory to store the generated dataset\n",
    "\n",
    "# Create a Papyrus object, which specifies the version and directory to store the payrus data\n",
    "papyrus = Papyrus(\n",
    "    data_dir=data_dir,\n",
    "    version=papyrus_version,\n",
    "    stereo=False,\n",
    "    plus_only=True,\n",
    ")\n",
    "\n",
    "# Create subset of payrus data for the given accession keys, returns a MoleculeTable\n",
    "mt = papyrus.getData(\n",
    "    dataset_name,\n",
    "    acc_keys,\n",
    "    quality,\n",
    "    output_dir=output_dir,\n",
    "    use_existing=False,\n",
    "    activity_types=[\"Ki\", \"IC50\", \"Kd\"]\n",
    ")\n",
    "mt.getDF().head()\n",
    "\n",
    "len(mt.getDF())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5a8bcf",
   "metadata": {},
   "source": [
    "## 2. QSPRpred API Wrapper\n",
    "\n",
    "We'll create a wrapper class for the QSPRpred API to handle identifier mapping requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d0bda33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PapyrusAPI Class\n",
    "# This class encapsulates the functionality to interact with the Papyrus database:\n",
    "\n",
    "class PapyrusAPI:\n",
    "    papyrus_version = \"latest\"  # Papyrus database version\n",
    "    data_dir = \"papyrus\"  # directory to store the Papyrus data\n",
    "    output_dir = \"data\"  # directory to store the generated dataset\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch_data(acc_keys, quality):\n",
    "        papyrus = Papyrus(\n",
    "                data_dir=data_dir,\n",
    "                version=papyrus_version,\n",
    "                stereo=False,\n",
    "                plus_only=True,\n",
    "            )\n",
    "        # create subset\n",
    "        mt = papyrus.getData(\n",
    "                dataset_name,\n",
    "                acc_keys,\n",
    "                quality,\n",
    "                output_dir=output_dir,\n",
    "                use_existing=False,\n",
    "                activity_types=[\"Ki\", \"IC50\", \"Kd\"]\n",
    "            )\n",
    "        if type(mt) is qsprpred.data.tables.mol.MoleculeTable:\n",
    "            return mt\n",
    "        else:\n",
    "             f\"Error: fetched data is {type(mt)}, should be MoleculeTable \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f9c459",
   "metadata": {},
   "source": [
    "## 3. Custom LangChain Tool\n",
    "\n",
    "Now, let's create a custom LangChain tool that uses our BridgeDbAPI wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "581d8ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define papyrus dataset handler   \n",
    "\n",
    "class PapyrusDatasetThresholdTool(BaseTool):\n",
    "    name: str =\"dataset_threshold\"\n",
    "    description: str = \"Used for determining if length of Papyrus dataset is higher than threshold\"\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "        # Parse the query; now expecting only three parts: acc_keys, quality, threshold\n",
    "        parts = query.split(\",\")\n",
    "        if len(parts) != 3:\n",
    "            return \"Error: Query should be in the format 'acc_keys, quality, threshold'\"\n",
    "        acc_keys, quality, threshold = [p.strip() for p in parts]\n",
    "        \n",
    "        # Attempt to determine if number of rows is higher than threshold\n",
    "        try:\n",
    "            papyrus_set = PapyrusAPI().fetch_data(acc_keys, quality)\n",
    "            num_rows = len(papyrus_set)\n",
    "            # return if number of rows is higher than threshold\n",
    "            return num_rows > float(threshold)\n",
    "            # print(result.getDF().Quality.value_counts())\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    def _arun(self, query: str) -> str:\n",
    "        # Async implementation (not needed for this tool)\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n",
    "    \n",
    "tools = [PapyrusDatasetThresholdTool()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74786418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b087eb7e65a045928b9a1160447ae4c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with a different ENSG identifier (e.g., BRCA2)\n",
    "tool = PapyrusDatasetThresholdTool()\n",
    "tool._run(\"P29275, high, 100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8959af25",
   "metadata": {},
   "source": [
    "## 4. AI Agent Configuration\n",
    "\n",
    "Let's set up our AI agent using the custom tool we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98758651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Is the number of rows higher than 1000 for papyrus set with acc key P29275 and quality high\n",
      "\n",
      "----\n",
      "----\n",
      "----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e762b2e5f1428ca0a8c7fed3ebae02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "Yes, the number of rows for the Papyrus dataset with accession key P29275 and high quality is higher than 1000.\n",
      "----\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def create_bridgedb_agent():\n",
    "    model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    tools = [PapyrusDatasetThresholdTool()]\n",
    "    memory = MemorySaver()\n",
    "    agent_executor = create_react_agent(model, tools, checkpointer=memory)\n",
    "    return agent_executor\n",
    "\n",
    "papyrus_agent = create_bridgedb_agent()\n",
    "\n",
    "def run_agent_query(query):\n",
    "    config = {\"configurable\": {\"thread_id\": \"qspr_conversation\"}}\n",
    "    \n",
    "    print(f\"Query: {query}\\n\")\n",
    "    for chunk in papyrus_agent.stream(\n",
    "        {\"messages\": [HumanMessage(content=query)]},\n",
    "        config\n",
    "    ):\n",
    "        if 'agent' in chunk and 'messages' in chunk['agent']:\n",
    "            for message in chunk['agent']['messages']:\n",
    "                if hasattr(message, 'content') and message.content:\n",
    "                    print(message.content)\n",
    "        print(\"----\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Test queries\n",
    "\n",
    "run_agent_query(\"Is the number of rows higher than 1000 for papyrus set with acc key P29275 and quality high\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
